/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com;

import java.io.FileWriter;
import java.io.IOException;
import java.net.MalformedURLException;
import java.net.URL;
import java.net.URLConnection;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.InputMismatchException;
import java.util.LinkedList;
import java.util.List;
import java.util.Queue;
import java.util.Scanner;
import java.util.Set;
import java.util.Vector;
import java.util.stream.Collectors;

import org.htmlparser.Parser;
import org.htmlparser.util.ParserException;

import com.Crawler;

import jdbm.helper.FastIterator;
import utils.LinkInfo;
import utils.LinkWordSort;
import utils.PageInfo;
import utils.Posting;
import utils.StopStem;

public class Driver {
	
	public static void main(String[] args) {
		showCLI();
		/*
		try {
			test();
		} catch (ParserException | IOException e) {
			e.printStackTrace();
		}
		*/
	}
	
	public static ArrayList<PageInfo> search(String q, WordIndex wi, LinkIndex li, LinkChildIndex lci, InvertedIndex ii, InvertedIndex ti, StopStem stemming) throws IOException {
		Searcher s = new Searcher(wi,li,lci,ii,ti,stemming);
		HashMap<Long, Double> scores = s.getQueryScore(q);
		if (scores == null)
			return null;
		ArrayList<Long> linkIds = LinkWordSort.sortLinks(scores);
		
		if (linkIds == null)
			return null;
		ArrayList<PageInfo> res = new ArrayList<PageInfo>();
		for (int i = 0; i < linkIds.size(); i++) {
			if (i>=50)
				break;
			long id = linkIds.get(i);
			double score = scores.get(id);			
			String url = li.getLink(id);
			LinkInfo info = (LinkInfo) li.getInfo(url);
			long date = info.date;
			long size = info.size;
			String title = info.title;
			
			HashMap<Long, Integer> wordFreq = new HashMap<Long, Integer>();
			if (ii.getWords(id)!=null) {	
				for (long wordId: ii.getWords(id)) {
					for (Posting p: (List<Posting>) ii.getPostings(wordId))
						if (p.doc == id) {
							wordFreq.put(wordId, p.pos.size());
							break;
						}
				}
			}
			if (ti.getWords(id)!=null) {
				for (long wordId: ti.getWords(id)) {
					for (Posting p: (List<Posting>) ti.getPostings(wordId))
						if (p.doc == id) {
							wordFreq.put(wordId, wordFreq.getOrDefault(wordId, 0) + p.pos.size());
							break;
						}
				}
			}
			ArrayList<Long> wordIds = LinkWordSort.sortWords(wordFreq);
			HashMap<String, Integer> freq = new HashMap<String, Integer>();
			if (wordIds != null) {
				for (int j = 0; j < wordIds.size(); j++)
					if (j>=10)
						break;
					else
						freq.put(wi.getWord(wordIds.get(j)), wordFreq.get(wordIds.get(j)));
			}
			ArrayList<String> parents = new ArrayList<String>();
			int count = 0;
			if (lci.getParent(url) != null) {
				for (String purl: lci.getParent(url))
					if (count >= 10)
						break;
					else {
						parents.add(purl);
						count++;
					}
			}
			ArrayList<String> children = new ArrayList<String>();
			count = 0;
			if (lci.getChild(url) != null) {
			for (String curl: (List<String>) lci.getChild(url))
				if (count >= 10)
					break;
				else {
					children.add(curl);
					count++;
				}
			}
			PageInfo pi = new PageInfo(score, url, date, size, title, freq, parents, children);
			res.add(pi);
		}
		return res;
	}
	
	private static void test() throws IOException, ParserException {
		

		WordIndex wi = new WordIndex("words", "wordHTree");
		LinkIndex li = new LinkIndex("links", "linkHTree");
		LinkChildIndex lci = new LinkChildIndex("child", "childHtree");
		InvertedIndex ii = new InvertedIndex("invert", "invertHTree");
		InvertedIndex ti = new InvertedIndex("title", "titleHTree");
		StopStem stemming = new StopStem("stopwords.txt");
		

		Searcher s = new Searcher(wi, li, lci, ii, ti, stemming);
		String test = "site ";
		
		
		HashMap<Long, Double> lenC = s.getDocContentLength();
		HashMap<Long, Double> lenT = s.getDocTitleLength();
		
		for (ArrayList<String> t: s.processQuery(test)) {
			System.out.println(t + ": " );
			HashMap<Long, Double> conScore, titScore;
			if (t.size()==1) {
				conScore = s.getWordContentScores(t.get(0));
				titScore = s.getWordTitleScores(t.get(0));
			} else {
				conScore = s.getTermContentScores(t);
				titScore = s.getTermTitleScores(t);
			}
			for (long id: conScore.keySet()) {
				System.out.println(id + "(content): " + conScore.get(id));
				System.out.println(id + "(title): " + titScore.get(id));
			}
		}
		
		for (long id: lenC.keySet()) {
			System.out.println(id + ": " + lenC.get(id) + " " + lenT.get(id));
		}
		
		HashMap<Long, Double> score = s.getQueryScore(test);
		for (long id: score.keySet()) {
			System.out.println(id + " " + li.getLink(id) + ": " + score.get(id));
		}

		ArrayList<PageInfo> res = search(test,wi,li,lci,ii,ti,stemming);
		System.out.println(res.get(0).score == 0.0);
			
	}
	
	public static void showCLI() {
		try {
			WordIndex wi = new WordIndex("words", "wordHTree");
			LinkIndex li = new LinkIndex("links", "linkHTree");
			LinkChildIndex lci = new LinkChildIndex("child", "childHtree");
			InvertedIndex ii = new InvertedIndex("invert", "invertHTree");
			InvertedIndex ti = new InvertedIndex("title", "titleHTree");
			StopStem stemming = new StopStem("stopwords.txt");
			
			long count = 30;
			Scanner inputScan = new Scanner(System.in);  // Create a Scanner object
			System.out.println("Databases loaded");
			System.out.println(li.size() + " pages are loaded into the system");


			//reset(wi,li,lci,ii);
			
			while (true) {
				int index = 0;
				while (true) {
					System.out.println("Enter \'1\' to index more pages into the system");
					System.out.println("Enter \'2\' to update the pages information in the system");
					System.out.println("Enter \'3\' to print the spider output text file");
					System.out.println("Enter \'4\' to clear all data in all databases");
					System.out.println("Enter \'5\' to save and quit");
					System.out.println("Enter \'6\' to search");
					try {
						index = inputScan.nextInt();
					}
					catch (InputMismatchException e)
					{
						System.out.println("Please enter a number");
						continue;
					}
					if (index<1 || index>6)
						System.out.println("Please enter a valid number");
					else break;
				}
				if (index==1) {
					int pages = 0;
					System.out.println("Please enter the number of pages to be fetched");
					while (true) {
						try {
							pages = inputScan.nextInt();
						}
						catch (InputMismatchException e)
						{
							System.out.println("Please enter a positive integer");
							continue;
						}
						if (pages<1)
							System.out.println("Please enter a positive integer");
						else break;
					}
					// call index page
					String initLink = "";
					inputScan.nextLine();
					System.out.println("Please enter the starting page");
					while (true) {
						initLink = inputScan.nextLine().trim();
						try {
							URLConnection connection = new URL(initLink).openConnection();
							Parser parser = new Parser(connection);
							if (connection.getContentType() != null && connection.getContentType().indexOf("text")==-1) {
								System.out.println("Please enter a page with text content");
								continue;
							}
							else break;
						}
						catch (ParserException | MalformedURLException e) {
							System.out.println("Please enter a valid page");
							continue; 					
						}
					}
					indexPage(pages, initLink, wi, li, lci, ii, ti, stemming);
				}
				else if (index == 2) 
					updateAllPage(wi,li,lci,ii,ti,stemming);
				else if (index == 3) 
					printResult(wi,li,lci,ii,ti);
				else if (index == 4) 
					reset(wi,li,lci,ii,ti);
				else if (index == 5) {
					finalize(wi,li,lci,ii,ti);
					break;
				}
				else {
					if (li.size() == 0) {
						System.out.println("Search function is disabled (NO page indexed). ");
						continue;
					}
					inputScan.nextLine();
					while (true) {
						System.out.println("Enter the query. Use double quotation marks to bound phrases");
						String q = inputScan.nextLine().trim();
						ArrayList<PageInfo> result = search(q,wi,li,lci,ii,ti,stemming);
						if (result == null) {
							System.out.println("Invalid query: unmatched quotation marks");
							continue;
						}
						for (PageInfo pi: result)
							System.out.println(pi);
						if (result.get(0).score == 0.0)
							System.out.println("All document(s) have 0 score. The ordering is arbitary.\n");
						break;
					}
					
				}
			}
			
			//indexPage(count-ii.size(), "http://www.cse.ust.hk/", wi,li,lci,ii);
			
			//printResult(wi,li,lci,ii);
			
			//finalize(wi,li,lci,ii);
		} catch (IOException e) {

			e.printStackTrace();
		}		
		
	}		
	
	private static void updateAllPage(WordIndex wi, LinkIndex li, LinkChildIndex lci, InvertedIndex ii, InvertedIndex ti, StopStem stemming) throws IOException{
		//todo
		FastIterator links = li.keys();
		ArrayList<String> keys = new ArrayList<String>(); 
		String tmpkey;
		while( (tmpkey = (String)links.next())!=null) {
			keys.add(tmpkey);
		}
		
		for (String key: keys) {
			try {
				URLConnection connection = new URL(key).openConnection();
				Parser parser = new Parser(connection);
				if (connection.getContentType() != null && connection.getContentType().indexOf("text")==-1) {
					long id = ((LinkInfo)li.getInfo(key)).id;
					li.deleteEntry(key);
					lci.delEntry(key);
					ii.delDoc(id);
					ti.delDoc(id);
				}
				else {
					Crawler crawl = new Crawler(key);
					long date = crawl.extractDate();
					long id = ((LinkInfo)li.getInfo(key)).id;
		
					if (date > ((LinkInfo)li.getInfo(key)).date) {
						
						lci.delEntry(key);
						ii.delDoc(id);
						ti.delDoc(id);
							
						long size = crawl.extractSize();
						String title = crawl.extractTitle();
						li.updateEntry(key, date, size, title);
						
						Vector<String> tmp = new Vector<String>();
						Vector<String> processTitle = new Vector<String>();
						for (String s: title.split("[ ]+"))
							tmp.add(s);
						tmp.forEach((n) -> processTitle.add(stemming.process(n.toLowerCase())));
						processTitle.removeIf((n) -> n.length()==0);
						HashMap<String, ArrayList<Integer>> posTitle = new HashMap<String, ArrayList<Integer>>();
						for (int i = 0; i < processTitle.size(); i++){
					           if (posTitle.get(processTitle.get(i)) == null){
					               ArrayList<Integer> p = new ArrayList<Integer>();
					               p.add(i);
					               posTitle.put(processTitle.get(i), p);
					           }
					           else{
					               ArrayList<Integer> p = posTitle.get(processTitle.get(i));
					               p.add(i);
					           }
					    }
						for (String word: posTitle.keySet()) {
							long wordId = (long) wi.addEntry(word);
							ti.addEntry(wordId, id, posTitle.get(word));
						}

						System.out.println("Crawling "+ key);
						
						Vector<String> crawlWord = crawl.extractWords();
						if (tmp.size()>0)
							for (int i = 0; i < tmp.size(); i++)	
								crawlWord.remove(0);
						
						Vector<String> processWord = new Vector<String>();
						crawlWord.forEach((n) -> processWord.add(stemming.process(n.toLowerCase())));
						processWord.removeIf((n) -> n.length()==0);
						HashMap<String, ArrayList<Integer>> pos = new HashMap<String, ArrayList<Integer>>();
						for (int i = 0; i < processWord.size(); i++){
					           if (pos.get(processWord.get(i)) == null){
					               ArrayList<Integer> p = new ArrayList<Integer>();
					               p.add(i);
					               pos.put(processWord.get(i), p);
					           }
					           else{
					               ArrayList<Integer> p = pos.get(processWord.get(i));
					               p.add(i);
					           }
					    }
						for (String word: pos.keySet()) {
							long wordId = (long) wi.addEntry(word);
							ii.addEntry(wordId, id, pos.get(word));
						}
						
						System.out.println("word crawling finish");
						
						Set<String> crawlLink = crawl.extractLinks();
						for (String newLink: crawlLink) {
							System.out.println("examining " + newLink);
							
							try {
								connection = new URL(newLink).openConnection();
								parser = new Parser(connection);
								if (connection.getContentType() != null && connection.getContentType().indexOf("text")==-1) {
									System.out.println("Non text, skipping");
									continue;
								}
							}
							catch (ParserException e) {
								System.out.println("Cert needed, skipping");
								continue; 					
							}		
							
							if (li.getInfo(newLink) == null)
								indexPage(1, newLink, wi, li, lci, ii, ti, stemming);	
						}
							
						lci.addEntry(key, new ArrayList<String>(crawlLink));
						
						System.out.println("link crawling finish");
						
					}
				}
			}
			catch (ParserException e) {
				long id = ((LinkInfo)li.getInfo(key)).id;
				li.deleteEntry(key);
				lci.delEntry(key);
				ii.delDoc(id);
				ti.delDoc(id);					
			}

		}	
	}

    
	private static void reset(WordIndex wi, LinkIndex li, LinkChildIndex lci, InvertedIndex ii, InvertedIndex ti) throws IOException {
		wi.deleteAll();
		ii.deleteAll();
		lci.deleteAll();
		li.deleteAll();
		ti.deleteAll();
	}
	
	public static void finalize(WordIndex wi, LinkIndex li, LinkChildIndex lci, InvertedIndex ii, InvertedIndex ti) throws IOException {
		wi.finalize();
		li.finalize();
		lci.finalize();
		ii.finalize();
		ti.finalize();
	}
	
	private static void printResult(WordIndex wi, LinkIndex li, LinkChildIndex lci, InvertedIndex ii, InvertedIndex ti) throws IOException {
		FileWriter myWriter = new FileWriter("spider_result.txt");
		
		FastIterator iter = li.keys();
		String key;
		SimpleDateFormat ft = new SimpleDateFormat ("yyyy-MM-dd");
		while( (key = (String)iter.next())!=null) {
			LinkInfo info = (LinkInfo) li.getInfo(key);
			if (info.crawled == false)
				continue;
			//System.out.println(info.title);
			//System.out.println(key);
			//System.out.println(ft.format(info.date) + ", " + info.size);
			
			myWriter.write(info.title+"\n");
			myWriter.write(key+"\n");
			myWriter.write(ft.format(info.date) + ", " + info.size+"\n");
			
			List<Long> wordIds = ii.getWords(info.id);
			if (wordIds != null)
				for (int i = 0; i < wordIds.size(); i++) {
					/*
					if (i > 9) {
						//System.out.println("...");
						myWriter.write("...\n");
						break;
					}
					*/
					String word = wi.getWord(wordIds.get(i));			
					
					List<Posting> posts = (List<Posting>) ii.getPostings(wordIds.get(i));
					for (Posting p: posts)
						if (p.doc == info.id) {
							//System.out.print(word + " " + p.freq + "; ");
							myWriter.write(word + " " + p.pos.size() + "; ");
							break;
						}
						
				}
			
			//System.out.println();
			myWriter.write("\n");
			
			List<String> child = (List<String>) lci.getChild(key);
			for (String c: child) {
				//System.out.println(c);
				myWriter.write(c+"\n");
			}
			
			//System.out.println("-------------------------------------------------------------------");
			myWriter.write("-------------------------------------------------------------------"+"\n");
		}
								
		//wi.printAll();
		//li.printAll();
		//ii.printAll();
		//lci.printAll();			

		myWriter.close();
	}
	
	private static void indexPage(long count, String initLink, WordIndex wi, LinkIndex li, LinkChildIndex lci, InvertedIndex ii, InvertedIndex ti, StopStem stemming) {

		try {
		Queue<String> links = new LinkedList<String>();
		Set<String> addLink = new HashSet<String>();
		links.offer(initLink);
		
		while (count > 0 && links.size() > 0) {
			if (addLink.size()>=count)
				break;
			Crawler crawl = new Crawler(links.peek());
			if (li.getInfo(links.peek())==null) {
				long date = crawl.extractDate();
				long size = crawl.extractSize();
				String title = crawl.extractTitle();
				LinkInfo info = (LinkInfo) li.addEntry(links.peek(), date, size, title);
				li.setCrawled(links.peek());
				
				Vector<String> tmp = new Vector<String>();
				Vector<String> processTitle = new Vector<String>();
				for (String s: title.split("[ ]+"))
					tmp.add(s);
				tmp.forEach((n) -> processTitle.add(stemming.process(n.toLowerCase())));
				processTitle.removeIf((n) -> n.length()==0);
				HashMap<String, ArrayList<Integer>> posTitle = new HashMap<String, ArrayList<Integer>>();
				for (int i = 0; i < processTitle.size(); i++){
			           if (posTitle.get(processTitle.get(i)) == null){
			               ArrayList<Integer> p = new ArrayList<Integer>();
			               p.add(i);
			               posTitle.put(processTitle.get(i), p);
			           }
			           else{
			               ArrayList<Integer> p = posTitle.get(processTitle.get(i));
			               p.add(i);
			           }
			    }
				for (String word: posTitle.keySet()) {
					long wordId = (long) wi.addEntry(word);
					ti.addEntry(wordId, info.id, posTitle.get(word));
				}
				
				System.out.println("Crawling "+ links.peek());
				
				Vector<String> crawlWord = crawl.extractWords();
				if (tmp.size()>0)
					for (int i = 0; i < tmp.size(); i++)	
						crawlWord.remove(0);
				
				Vector<String> processWord = new Vector<String>();
				crawlWord.forEach((n) -> processWord.add(stemming.process(n.toLowerCase())));
				processWord.removeIf((n) -> n.length()==0);
				HashMap<String, ArrayList<Integer>> pos = new HashMap<String, ArrayList<Integer>>();
				for (int i = 0; i < processWord.size(); i++){
			           if (pos.get(processWord.get(i)) == null){
			               ArrayList<Integer> p = new ArrayList<Integer>();
			               p.add(i);
			               pos.put(processWord.get(i), p);
			           }
			           else{
			               ArrayList<Integer> p = pos.get(processWord.get(i));
			               p.add(i);
			           }
			    }
				for (String word: pos.keySet()) {
					long wordId = (long) wi.addEntry(word);
					ii.addEntry(wordId, info.id, pos.get(word));
				}
				
				/*
				HashMap<String, Integer> wordFreq = new HashMap<String, Integer>();
				crawlWord.stream().collect(Collectors.groupingBy(s -> s)).forEach((k, v) -> wordFreq.put(k, v.size()));
				for (String word: wordFreq.keySet()) {
					long wordId = (long) wi.addEntry(word);
					ii.addEntry(wordId, info.id, wordFreq.get(word));
				}
				*/
				
				addLink.add(links.peek());
				
				System.out.println("word crawling finish");
			}
			Set<String> crawlLink = crawl.extractLinks();
			for (String link: crawlLink) {
				
				//System.out.println("queue size " + links.size());
				
				System.out.println("examining " + link);
				
				try {
					URLConnection connection = new URL(link).openConnection();
					Parser parser = new Parser(connection);
					if (connection.getContentType() != null && connection.getContentType().indexOf("text")==-1) {
						System.out.println("Non text, skipping");
						continue;
					}
				}
				catch (ParserException e) {
					System.out.println("Cert needed, skipping");
					continue; 					
				}
				
				
				
				if (li.getInfo(link) == null && links.contains(link)==false)
					links.offer(link);				

			}
			lci.addEntry(links.peek(), new ArrayList<String>(crawlLink));
			
			System.out.println("link crawling finish");
			
			links.poll();
		}
	}
		catch (IOException|ParserException e)
		{
			e.printStackTrace();
	}
	}
	
}
